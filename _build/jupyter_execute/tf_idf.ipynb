{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Vocabulary in *EarlyPrint* using Tf-Idf\n",
    "\n",
    "by [John R. Ladd](https://jrladd.com)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial, you'll learn how to examine the vocabulary in *EarlyPrint* texts using Tf-Idf: Term Frequencyâ€“Inverse Document Frequency. This technique weights words according to how many times they are used in one text relative to how often they're used across an entire corpus.\n",
    "\n",
    "Let's say, for example, that the word \"temperance\" appears about 18 times in the *The Faerie Queene*. Is that a lot or a little? How can we understand this wordcount not only in the context of Spenser's usage, but in the context of the word's appearance across a contemporary corpus? To understand whether this frequency is out of the ordinary, we need to know a word's **term frequency** (how many times does \"temperance\" appear in *The Faerie Queene*) as well as its **document frequency** (how many documents does \"temperance\" appear in at all). No doubt the word \"the\" appears in *The Faerie Queene* many many times (even in the title!), but it also appears in every single other English text. The fact that Spenser uses \"the\" a lot may not tell us much, but the fact that he uses \"temperance\" a lot could tell us something. Tf-Idf combines term frequency and document frequency into a score that can show us what words to pay attention to. It allows us to look at the frequencies of all the words in a single text highlighted against their presence or absence in a set of texts.\n",
    "\n",
    "Because Tf-Idf gives us more information than raw term frequency, it's a common ingredient in machine learning, natural language processing, and text classification tasks. *EarlyPrint*'s own [Discovery Engine](https://earlyprint.org/lab/tool_discovery_engine.html) uses Tf-Idf as one of several measures for determining text similarity in our corpus. For much more information on what Tf-Idf is and how to use it, please refer to [Matt Lavin's *Programming Historian* tutorial on Tf-Idf](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf), which was part of the inspiration for this post. However, it's important to remember that Tf-Idf is a heuristic, not a statistic. It's a simple, interpretable weighting system that gives a first sense of a word's relative importance or uniqueness. As Lavin and many others have suggested, Tf-Idf should therefore primarily be used as a first step in a research investigation, a way to ask new and interesting questions, rather than as a source of definitive claims. For more on the range of statistical techniques that can be used to investigate a word's relative prominence, start with this [blog post by Ted Underwood](https://tedunderwood.com/2011/11/09/identifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method/).\n",
    "\n",
    "While Lavin's tutorial will get you very far with most modern texts, early modern printed texts have irregular spellings and other inconsistencies that may throw off word counts (i.e. We need the computer to know that \"dog\" and \"dogge\" are the same word). In this tutorial, you'll learn how to use *EarlyPrint*'s linguistically annotated texts to deal with those irregularities and produce more reliable Tf-Idf results.\n",
    "\n",
    "## Tools\n",
    "\n",
    "Like the other tutorials in this series, this post presumes some introductory knowledge of Python. However, you need not write any code yourself. You can simply download this notebook and modify it to run on your own subcorpus of *EarlyPrint* texts.\n",
    "\n",
    "For this notebook to work, you will need to have Python3 installed as well as the following libraries:\n",
    "\n",
    "- [Jupyter Notebook](https://jupyter.org/install) (for running and reading the notebook)\n",
    "- [lxml](https://lxml.de/) (for parsing the XML)\n",
    "- [pandas](https://pandas.pydata.org/) (for organizing and displaying data)\n",
    "- [scikit-learn](https://scikit-learn.org/stable/) (for running Tf-Idf)\n",
    "\n",
    "Now let's get started!\n",
    "\n",
    "## Step 1: Import Libraries\n",
    "\n",
    "Once you've installed the necessary libraries, you need to **import** them into our current script. Often you don't need to import the whole library, just specific commands **from** that library. Our imports look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first two libraries are built in to Python, so we didn't need to install them\n",
    "import glob\n",
    "from collections import Counter\n",
    "from lxml import etree # This is the only part of lxml we need\n",
    "import pandas as pd # Import the entire pandas library, but use 'pd' as its nickname\n",
    "from sklearn.feature_extraction.text import TfidfTransformer # Import only the Tf-Idf tool from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Open some *EarlyPrint* texts and get all the regularized tokens\n",
    "\n",
    "As a sample, I've selected all the *EarlyPrint* texts available that were published in the year 1666. I did this because I'd like to find out about the distinctive vocabulary used in Margaret Cavendish's *Observations upon Experimental Philosophy* with *The Blazing World*, published in that year. But this method would work just as well for any grouping of texts.\n",
    "\n",
    "In the current *EarlyPrint* repository of freely downloadable texts, there are 143 documents that were published in 1666. Using that set of texts, this script ran on a standard laptop in just three or four minutes. We recommend that you start with a corpus of similar size to get used to the process, at a max of, say, 250 texts. This code *will* run on thousands or tens of thousands of texts at a time, though it may take a very long time on your average laptop.\n",
    "\n",
    "Here's the wonderful part: *EarlyPrint* texts are already tokenized (split up into individual words) and regularized (marked up with modernized spellings). So for our purposes here we simply need to open a file, get all its `<w>` tags for individual words, look to see if there is a regularized spelling, and put all of those words into a list. That may sound complicated, but the truth is that much of the hard work has already been done for us!\n",
    "\n",
    "I've collected my subcorpus in a folder called `1666_texts`. So I can process every file in that folder, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First you need a list of all files in your directory\n",
    "files = glob.glob(\"1666_texts/*.xml\") # THIS IS THE LINE YOU SHOULD MODIFY TO POINT AT THE TEXTS ON YOUR COMPUTER\n",
    "\n",
    "# Create an empty lists to put all our texts into\n",
    "all_tokenized = []\n",
    "\n",
    "# Then you can loop through the files\n",
    "for f in files:\n",
    "    parser = etree.XMLParser(collect_ids=False) # Create a parse object that skips XML IDs (in this case they just slow things down)\n",
    "    tree = etree.parse(f, parser) # Parse each file into an XML tree\n",
    "    xml = tree.getroot() # Get the XML from that tree\n",
    "    \n",
    "    # Now we can use lxml to find all the w tags       \n",
    "    word_tags = xml.findall(\".//{*}w\")\n",
    "    # In this next line you'll do several things at once to create a list of words for each text\n",
    "    # 1. Loop through each word: for word in word_tags\n",
    "    # 2. Make sure the tag has a word at all: if word.text != None\n",
    "    # 3. Get the regularized form of the word: word.get('reg', word.text)\n",
    "    # 4. Make sure all the words are in lowercase: .lower()\n",
    "    words = [word.get('reg', word.text).lower() for word in word_tags if word.text != None]\n",
    "    # Then we add these results to a master list\n",
    "    all_tokenized.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the affordances of the annotated *EarlyPrint* texts, we were able to skip a bunch of steps above. Accurate tokenization is handled by the `<w>` tags, and because there is a separate `<pc>` tag for punctuation, we don't have to worry about filtering that out either. By accessing the `<w>` tags directly and extracting the available regularized forms, we're now ready to move on to counting words.\n",
    "\n",
    "## Step 3: Counting Words\n",
    "\n",
    "In [Lavin's tutorial](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf), he uses the TfIdfVectorizer tool to tokenize and count words all together. Because our words are pretokenized, we can't combine these two steps. Instead, we need to make our own counts. We do this using a built-in method called `Counter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can count all the words in each text in one line of code\n",
    "all_counted = [Counter(a) for a in all_tokenized]\n",
    "\n",
    "# To prepare this data for Tf-Idf Transformation, we need to put into a different form, a DataFrame, using pandas.\n",
    "df = pd.DataFrame(all_counted, index=[f.split(\"/\")[1].split(\".\")[0] for f in files]).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Tf-Idf\n",
    "\n",
    "Now that we have our word counts, we can use scikit-learn's `TfidfTransformer` function to run Tf-Idf across our entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1f12fc66d2dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Once we've created the instance, we can \"transform\" our counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Make results readable using Pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1430\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mmatrix\u001b[0m \u001b[0mof\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \"\"\"\n\u001b[0;32m-> 1432\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0mdtype_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype_numeric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mresult_type\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "# First we need to create an \"instance\" of the transformer, with the proper settings.\n",
    "# We need to make sure that normalization is turned off\n",
    "tfidf = TfidfTransformer(norm=None, sublinear_tf=True)\n",
    "# I am choosing to turn on sublinear term frequency scaling, which takes the log of\n",
    "# term frequencies and can help to de-emphasize function words like pronouns and articles. \n",
    "# You might make a different choice depending on your corpus.\n",
    "\n",
    "# Once we've created the instance, we can \"transform\" our counts\n",
    "results = tfidf.fit_transform(df)\n",
    "\n",
    "# Make results readable using Pandas\n",
    "readable_results = pd.DataFrame(results.toarray(), index=df.index, columns=df.columns) # Convert information back to a DataFrame\n",
    "\n",
    "# Make the DataFrame columns the texts, and sort the DataFrame by \n",
    "# the words with the highest TF-IDF scores in the Cavendish text\n",
    "# Use .head(30) to show only the top 30 terms\n",
    "readable_results.T.sort_values(by=[\"A53049\"], ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What did we learn about *Observations Upon Experimental Philosophy* and *The Blazing World*?\n",
    "\n",
    "As I said before, Tf-Idf is a great way to generate research questions. In the chart above we can get a sense of some of the words Cavendish is using that aren't popular in other texts published that year.\n",
    "\n",
    "We would naturally expect that character names and other terms specific to this book would score high, and certainly we see \"empress,\" \"bird-men,\" \"worm-men,\" and of course \"blazing-world.\" It's exciting, however, that these aren't the words that score highest. This may be not *so* surprising since the characters and figures of *The Blazing World* are mixed in with the philosophical and scientific terminology from the rest of the text, but the resulting list nonetheless includes many interesting words.\n",
    "\n",
    "The first thing these results make me want to do is look more into Cavendish's use of terms that begin with \"self-\". Does she use the terms \"self-motion\" and \"self-knowledge\" commonly in her other works? What other writers are using these terms in the Restoration? Are other writers using the same terms but without the hyphenation? Are these terms specific to philosophical or scientific texts? All of these are directions to move in for future research, on just a single group of terms.\n",
    "\n",
    "I also want to note and investigate the paired terms that come up in this list, especially exterior and interior, corporeal and incorporeal, and animate and inanimate. My next approach might be to find a way to examine pairs of opposed terms together.\n",
    "\n",
    "A possible complaint about my approach here may be that I've not separated the words in *Observations upon Experimental Philosophy* from the words in *The Blazing World*. It's true that I've chosen to treat the physical book as the main unit of analysis here, but thanks to the XML markup available in these texts, you could fairly easily subdivide books into various texts and analyze them separately.\n",
    "\n",
    "Finally, it's perhaps unsurprising that Cavendish's interest in \"perception\" floats to the top of this list. However, we can note that different forms of the word, including \"perceptions,\" appear here. We might want to treat all forms of a word as a single term, and we can do so through a process called **lemmatization**. If we want to lemmatize our words before running Tf-Idf, there's more good news: *EarlyPrint* provides lemmas for all words just as it provides regularized forms. We could refactor our code to use the \"lemma\" attribute if we chose to, and all the versions of \"perception\" would be counted as a single term. \\[n.b. This will work for \"perceptions\" but not for \"perceptive.\" Lemmas revert to something like a dictionary headword, and so they usually retain their part of speech.\\]\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "I hope that this tutorial will not only help you to run Tf-Idf on texts you care about but that it will convince you of the usefulness of Tf-Idf as a technique. Texts derived from early modern printed books present unique challenges to linguistic analyses like Tf-Idf that rely on consistent spelling, but the regularized texts in *EarlyPrint* can assist in overcoming some of those challenges.\n",
    "\n",
    "If you have any questions, feel free to [contact me](mailto:jrladd@northwestern.edu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}