
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Text Classification: Supervision &#8212; EarlyPrint + Python</title>
    
  <link rel="stylesheet" href="_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Text Classification: Unsupervised Clustering" href="unsupervised.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/eplogo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">EarlyPrint + Python</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   EarlyPrint + Python
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="ep_xml.html">
   Parsing
   <em>
    EarlyPrint
   </em>
   XML Texts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="metadata.html">
   Working with Metadata, Creating Text Networks
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Text Analysis
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="tf_idf.html">
   Exploring Vocabulary Using Tf-Idf
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="similarity.html">
   Text Similarity
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Models and Classifiers
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="word2vec.html">
   Word Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unsupervised.html">
   Text Classification: Unsupervised Clustering
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Text Classification: Supervision
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/supervised.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/earlyprint/jupyterbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/earlyprint/jupyterbook/issues/new?title=Issue%20on%20page%20%2Fsupervised.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/earlyprint/jupyterbook/master?urlpath=tree/supervised.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reading-a-text-corpus">
   Reading a Text Corpus
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#targets">
   Targets
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supervised-classification">
   Supervised Classification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assessing-a-classifier">
   Assessing a Classifier
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="text-classification-supervision">
<h1>Text Classification: Supervision<a class="headerlink" href="#text-classification-supervision" title="Permalink to this headline">¶</a></h1>
<p>In the <a class="reference external" href="https://earlyprint.org/jupyterbook/unsupervised.html">previous tutorial</a>, we attempted to use an unsupervised method, K-means clustering, to create groups or clusters of our 1666 texts into genres. But we concluded that clustering might not be the ideal method for our data.</p>
<p>Thankfully, unsupervised methods aren’t the only way we can classify texts. We can use <em>supervision</em>, or <em>supervised classification</em>, to sort our texts into groups based on our knowledge of existing categories. Essentially, we give the classifier some texts with categories already labeled and ask it to guess at categories for the unlabeled data. The term “machine learning” is most often used to describe to this process, because the classifier “learns” the categories based on the labeled samples.</p>
<p>The supervised classifier we’ll use is one of the simplest and oldest: <a class="reference external" href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>, or logit. It’s mathematically related to the common statistical technique of <a class="reference external" href="https://en.wikipedia.org/wiki/Linear_regression">linear regression</a>. Where linear regression can predict <em>values</em>, logistic regression can predict <em>categories</em> (in other words: classify). While traditional logistic regression is a binary classifier, predicting whether data belongs in one of two categories, we can use multinomial logit to predict things in more than two categories.</p>
<p>We begin by importing the necessary libraries and functions. These are similar to our imports for unsupervised clustering, but with different libraries and models from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># General Libraries Needed</span>
<span class="kn">import</span> <span class="nn">glob</span><span class="o">,</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="nn">lxml</span> <span class="kn">import</span> <span class="n">etree</span>

<span class="c1"># Functions for Supervised Classification</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Libraries for Graphing</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="reading-a-text-corpus">
<h2>Reading a Text Corpus<a class="headerlink" href="#reading-a-text-corpus" title="Permalink to this headline">¶</a></h2>
<p>This first step is identical to what we did in the last tutorial, loading our corpus of 1666 texts and getting Tf-Idf values for each word. Here’s an important note from the last lesson:</p>
<blockquote>
<div><p>In previous tutorials, we extracted lists of words from the XML files and converted them to Tf-Idf values using <code class="docutils literal notranslate"><span class="pre">TfidfTransformer</span></code>. That would still work in this case, but in order to place some additional limits on Tf-Idf, we’ll convert the text from our XML back into strings. We can put those strings into <code class="docutils literal notranslate"><span class="pre">TfIdfVectorizer</span></code>, which will give us access to those additional limits (see below). The next code block extracts all the lemmas from our texts and converts them into strings.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use the glob library to create a list of file names</span>
<span class="n">filenames</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;1666_texts_full/*.xml&quot;</span><span class="p">)</span>
<span class="c1"># Parse those filenames to create a list of file keys (ID numbers)</span>
<span class="c1"># You&#39;ll use these later on.</span>

<span class="n">filekeys</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">filenames</span><span class="p">]</span>
<span class="c1"># Create an empty lists to put all our texts into</span>
<span class="n">all_tokenized</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">all_strings</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">nsmap</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;tei&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.tei-c.org/ns/1.0&#39;</span><span class="p">}</span>
<span class="n">parser</span> <span class="o">=</span> <span class="n">etree</span><span class="o">.</span><span class="n">XMLParser</span><span class="p">(</span><span class="n">collect_ids</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># Create a parse object that skips XML IDs (in this case they just slow things down)</span>

<span class="c1"># Then you can loop through the files</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">filenames</span><span class="p">:</span>
    <span class="n">tree</span> <span class="o">=</span> <span class="n">etree</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">parser</span><span class="p">)</span> <span class="c1"># Parse each file into an XML tree</span>
    <span class="n">xml</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">getroot</span><span class="p">()</span> <span class="c1"># Get the XML from that tree</span>
    <span class="c1"># Now we can use lxml to find all the w tags</span>
    <span class="c1"># In this next line you&#39;ll do several things at once to create a list of words for each text</span>
    <span class="c1"># 1. Loop through each word: for word in word_tags</span>
    <span class="c1"># 2. Make sure the tag has a word at all: if word.text != None</span>
    <span class="c1"># 3. Get the lemmatized form of the word: word.get(&#39;reg&#39;, word.text)</span>
    <span class="c1"># 4. Make sure all the words are in lowercase: .lower()</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;lemma&#39;</span><span class="p">,</span> <span class="n">word</span><span class="o">.</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">xml</span><span class="o">.</span><span class="n">iter</span><span class="p">(</span><span class="s2">&quot;{*}w&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">text</span> <span class="o">!=</span> <span class="kc">None</span><span class="p">]</span>
    <span class="n">full_string</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="c1"># Then we add these results to a master list</span>
    <span class="n">all_strings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">full_string</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And now we can calculate Tf-Idf, just as we did last time:</p>
<blockquote>
<div><p>Now that we have strings for each text, we can “vectorize” them into Tf-Idf values. Scikit-learn provides <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer">many options and parameters</a> for its <code class="docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code> that aren’t available in the <code class="docutils literal notranslate"><span class="pre">TfidfTransformer</span></code> we used in previous tutorials. Specifically, we want to use the <code class="docutils literal notranslate"><span class="pre">min_df</span></code> parameter to set the minimum document frequency to 2: this will filter out all words that appear in fewer than two texts. This creates a smaller list of features and will allow our models to run more quickly and more accurately.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First we need to create an &quot;instance&quot; of the vectorizer, with the proper settings.</span>
<span class="c1"># Normalization is set to &#39;l2&#39; by default</span>
<span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">sublinear_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># I am choosing to turn on sublinear term frequency scaling, which takes the log of</span>
<span class="c1"># term frequencies and can help to de-emphasize function words like pronouns and articles. </span>
<span class="c1"># You might make a different choice depending on your corpus.</span>

<span class="c1"># Once we&#39;ve created the instance, we can &quot;transform&quot; our counts</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">all_strings</span><span class="p">)</span>

<span class="c1"># Make results readable using Pandas</span>
<span class="n">readable_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">filekeys</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">tfidf</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span> <span class="c1"># Convert information back to a DataFrame</span>
<span class="n">readable_results</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>00</th>
      <th>01</th>
      <th>02</th>
      <th>03</th>
      <th>04</th>
      <th>05</th>
      <th>06</th>
      <th>08</th>
      <th>09</th>
      <th>10</th>
      <th>...</th>
      <th>zodiac</th>
      <th>zoilus</th>
      <th>zona</th>
      <th>zonara</th>
      <th>zone</th>
      <th>zophar</th>
      <th>zoroaster</th>
      <th>zosimus</th>
      <th>zouch</th>
      <th>zwinglius</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>B02845</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>A32444</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>A51130</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.017101</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>A89320</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.059049</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>A36358</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.011049</td>
      <td>...</td>
      <td>0.028335</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>A57156</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.033300</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>A57634</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>A65985</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>B03763</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.041815</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>A41955</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.018557</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>269 rows × 31385 columns</p>
</div></div></div>
</div>
<p>Now we have a dataset that tells us about the features of every text in our corpus!</p>
</div>
<div class="section" id="targets">
<h2>Targets<a class="headerlink" href="#targets" title="Permalink to this headline">¶</a></h2>
<p>Before we can begin using a supervised method, we need to add one more piece of data to our matrix. In addition to <em>samples</em> and <em>features</em>, matrices can also have <em>targets</em>, i.e. category labels that particular samples fall into. These targets can be used to <em>train</em> a classifier by telling it what to expect.</p>
<p>In this case, we need targets that label the genre of a text. Rather than assigning genre labels ourselves, we can pull targets from the <a class="reference external" href="https://earlyprint.org/jupyterbook/metadata.html">metadata</a>. As I mentioned in the last section, the logit method we’ll use can handle multiple categories at once, so let’s choose a couple different generic categories. We’ll use two of the most common subject labels in <em>EarlyPrint</em>: history and poetry. We’ll also have a third “neither” category for texts that don’t belong to either group.</p>
<p><em>n.b. This is a flawed example! In a real research situation you may want your categories to be more precise, and you may want a more apples-to-apples comparison than history and poetry, which are very different. And “history” itself is a very fuzzy category that may include different kinds of texts! You may also want to avoid a catch-all “neither” category. However, for the purposes of <strong>learning the method</strong>, it helps to have categories that are very different.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">targets</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">filekey</span> <span class="ow">in</span> <span class="n">filekeys</span><span class="p">:</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;../../epmetadata/header/</span><span class="si">{</span><span class="n">filekey</span><span class="si">}</span><span class="s1">_header.xml&#39;</span> <span class="c1"># Get TCP ID from filename</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="n">etree</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">parser</span><span class="p">)</span> <span class="c1"># Create lxml tree for metadata</span>
    <span class="c1"># Find all the keywords in each text</span>
    <span class="n">keywords</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">metadata</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s2">&quot;.//tei:item&quot;</span><span class="p">,</span> <span class="n">namespaces</span><span class="o">=</span><span class="n">nsmap</span><span class="p">)]</span>
    <span class="c1"># Search in those keywords for the word &quot;sermon&quot; or words pertaining to poetry</span>
    <span class="n">poetry_terms</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;poetry&#39;</span><span class="p">,</span> <span class="s1">&#39;broadside poems&#39;</span><span class="p">,</span> <span class="s1">&#39;poems&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">k</span> <span class="ow">in</span> <span class="n">poetry_terms</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keywords</span><span class="p">):</span>
        <span class="n">targets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;poetry&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">any</span><span class="p">(</span><span class="s1">&#39;history&#39;</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keywords</span><span class="p">):</span>
        <span class="n">targets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;history&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">targets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;neither&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Counter</span><span class="p">(</span><span class="n">targets</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;poetry&#39;, &#39;history&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;poetry&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;history&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;history&#39;, &#39;poetry&#39;, &#39;history&#39;, &#39;history&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;poetry&#39;, &#39;history&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;history&#39;, &#39;poetry&#39;, &#39;history&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;history&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;history&#39;, &#39;history&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;poetry&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;neither&#39;, &#39;history&#39;, &#39;neither&#39;, &#39;poetry&#39;, &#39;neither&#39;, &#39;neither&#39;]
Counter({&#39;neither&#39;: 162, &#39;history&#39;: 58, &#39;poetry&#39;: 49})
</pre></div>
</div>
</div>
</div>
<p>We can see that <code class="docutils literal notranslate"><span class="pre">targets</span></code> is simply a list of labels for each text in our corpus. Importantly, the list is in the same order as the texts in our Tf-Idf matrix.</p>
<p>If we count the categories up, we can see that 49 of our texts contain poetry, 58 of our texts are labeled with history, and 162 have neither label. (When texts were labeled both “poetry” and “history,” I kept the label “poetry.” The actual counts of poetry and historical texts in this corpus are probably larger, but this is what we can find using the existing subject headings.)</p>
</div>
<div class="section" id="supervised-classification">
<h2>Supervised Classification<a class="headerlink" href="#supervised-classification" title="Permalink to this headline">¶</a></h2>
<p>Now that we have target labels, we can use them to train a <em>supervised classifier</em> to determine genre categories. Unlike with K-means clustering, where we simply create a model and plug in the entire dataset, we need to split our data into a <em>training set</em>, which we use to help our model learn, and a <em>test set</em>, which we use to see how the model did. In our case, we’ll split our data approximately in half, using just over half of the plays for training and reserving the rest for testing.</p>
<p>We need to split both the feature set (denoted by a capital X) and the target labels (denoted by a lowercase y). Luckily, scikit-learn does all of this for us with its <code class="docutils literal notranslate"><span class="pre">train_test_split()</span></code> function.</p>
<p>Once the data is split, we can choose a model to train. In this case, the method I’ve chosen is logistic regression (logit). As I mentioned before, logit is quite an old method for classification, and it is useful in part because it is easy to explain and provides results that (as we shall see) are easier to interpret than newer methods like neural networks. Logistic regression uses a logistic function to draw an s-shaped <a class="reference external" href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid curve</a> that takes any value and converts it to a value between 0 and 1. The closer the value is to 0 or 1, the more closely it belongs in one category or another. Because of this 0-or-1, one-or-the-other feature, logit was originally only a <em>binary</em> classifier: it could only tell if something was in one of just two categories. However, we can use multiclass or multinomial logistic regression, and the model will predict all three of our classes at once.</p>
<p>In the code below, we’ll split the data automatically, create a logistic regression model, and “fit” that model using the training data. Then, we’ll run the model to <em>predict</em> categories for the texts in the test set. In the end, we can get accuracy scores, as well as a list of the texts in the test set with their real and predicted genres.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">readable_results</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.45</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># evaluate accuracy</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy score:&quot;</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">))</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Results of this run:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Play Title | Actual Genre | Predicted Genre&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">title</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">predicted</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">real</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">predicted</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy score: 0.7540983606557377

Results of this run:

Play Title | Actual Genre | Predicted Genre
A41058 | neither | neither
A23770 | neither | neither
A71231 | neither | neither
B05875 | poetry | poetry
A37237 | history | neither
A47095 | neither | neither
A50075 | history | neither
B03489 | poetry | poetry
B04154 | poetry | poetry
A23912 | neither | neither
B06375 | poetry | poetry
B03763 | neither | neither
A67335 | poetry | poetry
A27397 | neither | neither
A41072 | neither | neither
A47379 | neither | neither
A41527 | neither | neither
A57634 | neither | neither
A48909 | neither | neither
A64918 | history | history
A60948 | neither | neither
A44478 | neither | poetry
A32751 | history | neither
A62436 | neither | neither
A67572 | neither | neither
A61956 | neither | neither
A86466 | neither | history
B04338 | poetry | neither
A32233 | neither | history
B03114 | poetry | poetry
A48218 | neither | neither
A55410 | neither | neither
A54418 | neither | neither
A39938 | neither | history
A46137 | history | history
A46196 | history | history
A64258 | neither | neither
A32503 | neither | history
A44938 | neither | neither
A52519 | poetry | poetry
B05479 | neither | history
A59229 | neither | neither
A43741 | neither | neither
A39345 | poetry | poetry
A95297 | history | neither
A38741 | history | history
A59325 | neither | poetry
A87330 | history | neither
A53911 | history | poetry
A63767 | neither | neither
A35114 | neither | neither
A37291 | neither | neither
A40151 | neither | neither
B04364 | poetry | poetry
A60606 | poetry | poetry
A35608 | history | neither
A47545 | history | history
B05318 | neither | neither
A59614 | neither | neither
A36272 | neither | neither
A32314 | history | history
A61867 | neither | neither
A29110 | neither | neither
A46152 | history | history
A44594 | neither | neither
A56039 | neither | neither
A81069 | history | poetry
B02123 | neither | history
A38792 | neither | neither
A37096 | history | neither
A29439 | neither | history
A66777 | poetry | neither
A39974 | neither | neither
A45206 | neither | neither
A47547 | neither | history
A65702 | neither | neither
A57421 | neither | neither
B04360 | poetry | poetry
A48797 | neither | neither
A66579 | neither | neither
B02089 | neither | history
A63952 | neither | history
A61207 | neither | neither
B06022 | poetry | poetry
B03631 | history | neither
A46193 | history | history
A91186 | history | neither
A50777 | neither | neither
A79623 | neither | neither
A51346 | poetry | poetry
B04701 | poetry | poetry
A50520 | neither | neither
A32559 | neither | history
A42533 | poetry | poetry
A32967 | neither | neither
A93280 | neither | neither
A46087 | history | history
A51130 | poetry | poetry
A61891 | neither | neither
A75960 | neither | neither
B03106 | poetry | poetry
A54070 | neither | neither
A93278 | poetry | poetry
A63951 | neither | history
A44879 | poetry | poetry
A32555 | history | history
A61206 | neither | neither
A31237 | history | neither
A41053 | neither | neither
A39482 | neither | neither
A77803 | neither | neither
A39442 | neither | neither
A80816 | neither | neither
A59168 | neither | neither
A53818 | neither | neither
A51877 | neither | history
A39246 | poetry | poetry
B05057 | poetry | neither
A41958 | neither | neither
A31229 | history | history
B03672 | poetry | poetry
A29017 | neither | neither
</pre></div>
</div>
</div>
</div>
<p>The results above show us a few things about how the logistic regression model did. First, the <em>accuracy score</em> shows the percentage of texts in the test set that were labeled correctly in this run of the model. 75% accuracy is pretty good for a first attempt!</p>
<p>And the results themselves show that the model got things mostly right. By browsing the list above, we can see that most of the expected and predicted values match.</p>
<p>But there’s a catch: the majority of our texts are in the “neither” category, i.e. they’re neither poetry nor history. If the model guesses “neither” most of the time, it’s likely to be right most of the time. The question, then, is: how well did the model <em>actually</em> do at identifying the texts in these very different categories? There are a few ways of finding out.</p>
</div>
<div class="section" id="assessing-a-classifier">
<h2>Assessing a Classifier<a class="headerlink" href="#assessing-a-classifier" title="Permalink to this headline">¶</a></h2>
<p>After training a classifier, we naturally want to know if it worked. One way we can do this is by scanning a list of results, as above. We can look at the texts that are listed and find out if the mistakes the model made were interesting: perhaps some of the texts are mislabeled and the model is right that they really <em>are</em> poetry or history. And it’s important to do this kind of close-reading check for any model. A humanities researcher may care as much about what a model gets wrong as what it gets right: the point isn’t training a computer to do a task for us, it’s what we learn about our corpus by going through the process of training.</p>
<p>And to that end, in addition to examining examples, there are a range of mathematical ways to assess a classifier. We already calculated an accuracy score in the code above, which tells us how often the classifier picked the correct category. However, this is only the accuracy result for the data split in just one way and run just once. How does the model do if we split up the data differently?</p>
<p>The <em>cross validation score</em> answers this question by running the model several times with differently split data. We can use a technique called <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation"><em>k</em>-fold cross-validation</a>, where <em>k</em> refers to any possible number. This method splits the data up differently <em>k</em> number of times and calculates the average accuracy across the differently-split runs of the classifier. Maybe in one random run, the data winds up split in such a way that the model is much more accurate than it is every other time it’s run. Cross-validation accounts for this and gives us a sense of how well the model does no matter how the data is split. We can see that the cross validation shows that this particular run of the model is fairly close to the expected result.</p>
<p>With <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>, running <em>k</em>-fold cross-validation is as simple as one line of code. We’ll do 5-fold cross-validation by setting <code class="docutils literal notranslate"><span class="pre">cv=5</span></code>. This means that <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> will run the model 5 different ways:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cross validation score:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">readable_results</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cross validation score: 0.7880503144654087
</pre></div>
</div>
</div>
</div>
<p>These results may put our minds somewhat at ease. The classifier actually performs slightly better in cross-validation than it did in our test run: about 79%. But we still don’t know how the model performed on our three different categories.</p>
<p>In addition to cross-validation, we can also assess a model’s accuracy using a <em>confusion matrix</em>, a chart that shows how often predicted values matched expected values. In the code below we’ll generate the confusion matrix for our model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
<span class="n">cm_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cm_df</span><span class="p">,</span><span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greens&#39;</span><span class="p">,</span><span class="n">linewidths</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff21c34d460&gt;
</pre></div>
</div>
<img alt="_images/supervised_14_1.png" src="_images/supervised_14_1.png" />
</div>
</div>
<p>In this confusion matrix, predicted values (the output of the model) are shown horizontally and expected values (what was in the original data) are shown vertically. When the two match, we can see the number of <em>true positives</em>, places where the predicted value matched the expected value. Those are the darker squares along the diagonal from top left to bottom right. Our model got history right 11 times, poetry right 21 times, and correctly predicted when a text was neither 60 times.</p>
<p>The confusion matrix also shows false results—places where the predicted value did not match the expected value. Like we saw in the list above, the model miscategorized 3 texts originally labeled “neither” as poetry, for example. This could be a blind spot in our model, or it could be that we have more poetry texts in our data than we originally thought! We would have to examine the texts to find out.</p>
<p>What we can see from this matrix is that the model struggled the most with the history category. Look at the top row and the left-most column, where the history texts are categorized. Things are less neatly sorted into the top-left square of true positives as we’d like to see. Instead, history texts are sometimes misclassified as “neither,” and “neither” texts are sometimes misclassified as history.</p>
<p>We can quantify what we’re seeing in the confusion matrix with the final way of assessing our classifier: <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>’s classification report. Here’s the classification report that goes with the above matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

     history       0.46      0.48      0.47        23
     neither       0.82      0.80      0.81        75
      poetry       0.84      0.88      0.86        24

    accuracy                           0.75       122
   macro avg       0.71      0.72      0.71       122
weighted avg       0.76      0.75      0.76       122
</pre></div>
</div>
</div>
</div>
<p>The classification report gives us scores for the <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">precision and recall</a> of the classifier across each category. Precision is the ratio of true positives (correct guesses of a text’s category) to the number of total times it guessed that category. Recall is the ratio of true positives to the total number of texts in that category that existed in the dataset. So precision tells us how often the model guessed correctly while recall tells us how often the model found the right thing—two different views of a similar task. (If this is still confusing to you, I highly recommend the examples and visualizations in the <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">Wikipedia article on this topic</a>.)</p>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall#F-measure">F1-score</a> combines these two measures, and “support” just shows the number of times that category appeared in our dataset (i.e. the total number of texts with that label).</p>
<p>How did our classifier do, according to this report? It did a decent job with the “neither” category, which makes sense since “neither” was the largest category. And it did really well with poetry! Especially with recall: if there was poetry in a text, our classifier could find it. Given that verse is distinctive compared to a nebulous genre like “history,” it makes sense that this was the classifier’s strength.</p>
<p>And we can can see that the history category is where the classifier really struggled. In fact, it’s what’s dragging down our overall accuracy score.  Both precision and recall are less than 50%: the classifier can neither find historical texts nor guess their label correctly. It’s worse than a coin flip!</p>
<p>The classification report clarifies trends that were harder to see in our other methods of model assessment. Namely, one category (history) is badly suited to this method. Perhaps we could create a classifier that’s quite good at identifying poetry texts, but we need to go back to the drawing board on history. This makes sense since, as I mentioned previously, “history” isn’t a very well-defined genre in this corpus. Instead it’s a subject label applied to lots of different kinds of texts. If we’re really interested in correctly classifying historical texts, we may want to hand label a sample and start over with our model.</p>
<p>Just as we discovered with K-means clustering, it’s easy to run these models but harder to assess and interpret their results. It requires a knowledge of the task at hand, a general idea of what’s in the corpus, and a willingness to drill down to specific examples. The example we worked through above didn’t work perfectly, and that makes it a good object lesson in supervised classification. You won’t always find the right model or the right categories on the first try, and that’s okay. By applying your knowledge of the corpus and some general principles of how classifiers work, you can still learn something from imperfect classifiers and work toward a model that does the job.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="unsupervised.html" title="previous page">Text Classification: Unsupervised Clustering</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By John R. Ladd<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>