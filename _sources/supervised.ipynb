{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification: Supervision\n",
    "\n",
    "In the [previous tutorial](https://earlyprint.org/jupyterbook/unsupervised.html), we attempted to use an unsupervised method, K-means clustering, to create groups or clusters of our 1666 texts into genres. But we concluded that clustering might not be the ideal method for our data.\n",
    "\n",
    "Thankfully, unsupervised methods aren't the only way we can classify texts. We can use *supervision*, or *supervised classification*, to sort our texts into groups based on our knowledge of existing categories. Essentially, we give the classifier some texts with categories already labeled and ask it to guess at categories for the unlabeled data. The term \"machine learning\" is most often used to describe to this process, because the classifier \"learns\" the categories based on the labeled samples.\n",
    "\n",
    "The supervised classifier we'll use is one of the simplest and oldest: [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), or logit. It's mathematically related to the common statistical technique of [linear regression](https://en.wikipedia.org/wiki/Linear_regression). Where linear regression can predict *values*, logistic regression can predict *categories* (in other words: classify). While traditional logistic regression is a binary classifier, predicting whether data belongs in one of two categories, we can use multinomial logit to predict things in more than two categories.\n",
    "\n",
    "We begin by importing the necessary libraries and functions. These are similar to our imports for unsupervised clustering, but with different libraries and models from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries Needed\n",
    "import glob, csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from lxml import etree\n",
    "\n",
    "# Functions for Supervised Classification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a Text Corpus\n",
    "\n",
    "This first step is identical to what we did in the last tutorial, loading our corpus of 1666 texts and getting Tf-Idf values for each word. Here's an important note from the last lesson:\n",
    "\n",
    ">In previous tutorials, we extracted lists of words from the XML files and converted them to Tf-Idf values using `TfidfTransformer`. That would still work in this case, but in order to place some additional limits on Tf-Idf, we'll convert the text from our XML back into strings. We can put those strings into `TfIdfVectorizer`, which will give us access to those additional limits (see below). The next code block extracts all the lemmas from our texts and converts them into strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the glob library to create a list of file names\n",
    "filenames = glob.glob(\"1666_texts_full/*.xml\")\n",
    "# Parse those filenames to create a list of file keys (ID numbers)\n",
    "# You'll use these later on.\n",
    "\n",
    "filekeys = [f.split('/')[-1].split('.')[0] for f in filenames]\n",
    "# Create an empty lists to put all our texts into\n",
    "all_tokenized = []\n",
    "all_strings = []\n",
    "\n",
    "nsmap={'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "parser = etree.XMLParser(collect_ids=False) # Create a parse object that skips XML IDs (in this case they just slow things down)\n",
    "\n",
    "# Then you can loop through the files\n",
    "for f in filenames:\n",
    "    tree = etree.parse(f, parser) # Parse each file into an XML tree\n",
    "    xml = tree.getroot() # Get the XML from that tree\n",
    "    # Now we can use lxml to find all the w tags\n",
    "    # In this next line you'll do several things at once to create a list of words for each text\n",
    "    # 1. Loop through each word: for word in word_tags\n",
    "    # 2. Make sure the tag has a word at all: if word.text != None\n",
    "    # 3. Get the lemmatized form of the word: word.get('reg', word.text)\n",
    "    # 4. Make sure all the words are in lowercase: .lower()\n",
    "    words = [word.get('lemma', word.text).lower() for word in xml.iter(\"{*}w\") if word.text != None]\n",
    "    full_string = ' '.join(words)\n",
    "    # Then we add these results to a master list\n",
    "    all_strings.append(full_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can calculate Tf-Idf, just as we did last time:\n",
    "\n",
    ">Now that we have strings for each text, we can \"vectorize\" them into Tf-Idf values. Scikit-learn provides [many options and parameters](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) for its `TfidfVectorizer` that aren't available in the `TfidfTransformer` we used in previous tutorials. Specifically, we want to use the `min_df` parameter to set the minimum document frequency to 2: this will filter out all words that appear in fewer than two texts. This creates a smaller list of features and will allow our models to run more quickly and more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zoilus</th>\n",
       "      <th>zona</th>\n",
       "      <th>zonara</th>\n",
       "      <th>zone</th>\n",
       "      <th>zophar</th>\n",
       "      <th>zoroaster</th>\n",
       "      <th>zosimus</th>\n",
       "      <th>zouch</th>\n",
       "      <th>zwinglius</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B02845</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A32444</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A51130</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A89320</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A36358</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A57156</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A57634</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A65985</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B03763</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A41955</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>269 rows Ã— 31385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         00   01   02   03   04   05   06   08   09        10  ...    zodiac  \\\n",
       "B02845  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.000000   \n",
       "A32444  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.000000   \n",
       "A51130  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.017101  ...  0.000000   \n",
       "A89320  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.059049  ...  0.000000   \n",
       "A36358  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.011049  ...  0.028335   \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  ...       ...  ...       ...   \n",
       "A57156  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.033300  ...  0.000000   \n",
       "A57634  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.000000   \n",
       "A65985  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...  0.000000   \n",
       "B03763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.041815  ...  0.000000   \n",
       "A41955  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.018557  ...  0.000000   \n",
       "\n",
       "        zoilus  zona  zonara  zone  zophar  zoroaster  zosimus  zouch  \\\n",
       "B02845     0.0   0.0     0.0   0.0     0.0        0.0      0.0    0.0   \n",
       "A32444     0.0   0.0     0.0   0.0     0.0        0.0      0.0    0.0   \n",
       "A51130     0.0   0.0     0.0   0.0     0.0        0.0      0.0    0.0   \n",
       "A89320     0.0   0.0     0.0   0.0     0.0        0.0      0.0    0.0   \n",
       "A36358     0.0   0.0     0.0   0.0     0.0        0.0      0.0    0.0   \n",
       "...        ...   ...     ...   ...     ...        ...      ...    ...   \n",
       "A57156     0.0   0.0     0.0   0.0     0.0        0.0      0.0    0.0   \n",
       "A57634     0.0   0.0     0.0   0.0     0.0        0.0      0.0    0.0   \n",
       "A65985     0.0   0.0     0.0   0.0     0.0        0.0      0.0    0.0   \n",
       "B03763     0.0   0.0     0.0   0.0     0.0        0.0      0.0    0.0   \n",
       "A41955     0.0   0.0     0.0   0.0     0.0        0.0      0.0    0.0   \n",
       "\n",
       "        zwinglius  \n",
       "B02845        0.0  \n",
       "A32444        0.0  \n",
       "A51130        0.0  \n",
       "A89320        0.0  \n",
       "A36358        0.0  \n",
       "...           ...  \n",
       "A57156        0.0  \n",
       "A57634        0.0  \n",
       "A65985        0.0  \n",
       "B03763        0.0  \n",
       "A41955        0.0  \n",
       "\n",
       "[269 rows x 31385 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we need to create an \"instance\" of the vectorizer, with the proper settings.\n",
    "# Normalization is set to 'l2' by default\n",
    "tfidf = TfidfVectorizer(min_df=2, sublinear_tf=True)\n",
    "# I am choosing to turn on sublinear term frequency scaling, which takes the log of\n",
    "# term frequencies and can help to de-emphasize function words like pronouns and articles. \n",
    "# You might make a different choice depending on your corpus.\n",
    "\n",
    "# Once we've created the instance, we can \"transform\" our counts\n",
    "results = tfidf.fit_transform(all_strings)\n",
    "\n",
    "# Make results readable using Pandas\n",
    "readable_results = pd.DataFrame(results.toarray(), index=filekeys, columns=tfidf.get_feature_names()) # Convert information back to a DataFrame\n",
    "readable_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dataset that tells us about the features of every text in our corpus!\n",
    "\n",
    "## Targets\n",
    "\n",
    "Before we can begin using a supervised method, we need to add one more piece of data to our matrix. In addition to *samples* and *features*, matrices can also have *targets*, i.e. category labels that particular samples fall into. These targets can be used to *train* a classifier by telling it what to expect.\n",
    "\n",
    "In this case, we need targets that label the genre of a text. Rather than assigning genre labels ourselves, we can pull targets from the [metadata](https://earlyprint.org/jupyterbook/metadata.html). As I mentioned in the last section, the logit method we'll use can handle multiple categories at once, so let's choose a couple different generic categories. We'll use two of the most common subject labels in *EarlyPrint*: history and poetry. We'll also have a third \"neither\" category for texts that don't belong to either group.\n",
    "\n",
    "*n.b. This is a flawed example! In a real research situation you may want your categories to be more precise, and you may want a more apples-to-apples comparison than history and poetry, which are very different. And \"history\" itself is a very fuzzy category that may include different kinds of texts! You may also want to avoid a catch-all \"neither\" category. However, for the purposes of **learning the method**, it helps to have categories that are very different.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poetry', 'history', 'poetry', 'neither', 'history', 'neither', 'history', 'neither', 'neither', 'neither', 'history', 'poetry', 'history', 'neither', 'poetry', 'neither', 'history', 'neither', 'neither', 'poetry', 'history', 'history', 'neither', 'neither', 'poetry', 'neither', 'neither', 'history', 'neither', 'neither', 'neither', 'poetry', 'neither', 'history', 'neither', 'neither', 'neither', 'neither', 'neither', 'neither', 'neither', 'history', 'neither', 'history', 'history', 'poetry', 'history', 'history', 'poetry', 'neither', 'neither', 'history', 'poetry', 'neither', 'neither', 'neither', 'poetry', 'neither', 'neither', 'neither', 'history', 'poetry', 'neither', 'neither', 'poetry', 'poetry', 'neither', 'neither', 'neither', 'history', 'neither', 'poetry', 'poetry', 'history', 'history', 'neither', 'neither', 'history', 'neither', 'neither', 'neither', 'history', 'neither', 'history', 'neither', 'neither', 'neither', 'neither', 'neither', 'neither', 'neither', 'neither', 'poetry', 'history', 'neither', 'neither', 'poetry', 'poetry', 'neither', 'neither', 'neither', 'poetry', 'neither', 'neither', 'neither', 'neither', 'history', 'history', 'neither', 'neither', 'neither', 'poetry', 'neither', 'neither', 'neither', 'poetry', 'neither', 'neither', 'poetry', 'neither', 'neither', 'neither', 'neither', 'poetry', 'history', 'neither', 'history', 'poetry', 'neither', 'neither', 'neither', 'neither', 'history', 'neither', 'neither', 'poetry', 'neither', 'neither', 'neither', 'poetry', 'poetry', 'neither', 'neither', 'neither', 'neither', 'neither', 'neither', 'neither', 'neither', 'history', 'neither', 'neither', 'neither', 'poetry', 'neither', 'neither', 'neither', 'neither', 'history', 'neither', 'history', 'poetry', 'neither', 'neither', 'poetry', 'neither', 'neither', 'poetry', 'neither', 'history', 'history', 'poetry', 'history', 'poetry', 'neither', 'history', 'neither', 'neither', 'neither', 'poetry', 'history', 'neither', 'history', 'history', 'neither', 'neither', 'neither', 'poetry', 'neither', 'neither', 'history', 'neither', 'neither', 'poetry', 'neither', 'history', 'neither', 'history', 'poetry', 'neither', 'neither', 'history', 'history', 'history', 'neither', 'neither', 'history', 'history', 'neither', 'history', 'neither', 'neither', 'history', 'neither', 'neither', 'neither', 'neither', 'neither', 'neither', 'neither', 'poetry', 'neither', 'poetry', 'neither', 'neither', 'neither', 'neither', 'poetry', 'neither', 'history', 'history', 'history', 'history', 'neither', 'poetry', 'poetry', 'history', 'neither', 'neither', 'neither', 'neither', 'neither', 'poetry', 'neither', 'neither', 'history', 'poetry', 'neither', 'neither', 'neither', 'neither', 'history', 'neither', 'poetry', 'history', 'neither', 'poetry', 'poetry', 'neither', 'neither', 'neither', 'neither', 'neither', 'neither', 'history', 'neither', 'poetry', 'neither', 'neither']\n",
      "Counter({'neither': 162, 'history': 58, 'poetry': 49})\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "\n",
    "for filekey in filekeys:\n",
    "    filename = f'../../epmetadata/header/{filekey}_header.xml' # Get TCP ID from filename\n",
    "    metadata = etree.parse(filename, parser) # Create lxml tree for metadata\n",
    "    # Find all the keywords in each text\n",
    "    keywords = [k.text.lower().strip('.') for k in metadata.findall(\".//tei:item\", namespaces=nsmap)]\n",
    "    # Search in those keywords for the word \"sermon\" or words pertaining to poetry\n",
    "    poetry_terms = ['poetry', 'broadside poems', 'poems']\n",
    "    if any(k in poetry_terms for k in keywords):\n",
    "        targets.append('poetry')\n",
    "    elif any('history' in k for k in keywords):\n",
    "        targets.append('history')\n",
    "    else:\n",
    "        targets.append('neither')\n",
    "\n",
    "print(targets)\n",
    "print(Counter(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `targets` is simply a list of labels for each text in our corpus. Importantly, the list is in the same order as the texts in our Tf-Idf matrix.\n",
    "\n",
    "If we count the categories up, we can see that 49 of our texts contain poetry, 58 of our texts are labeled with history, and 162 have neither label. (When texts were labeled both \"poetry\" and \"history,\" I kept the label \"poetry.\" The actual counts of poetry and historical texts in this corpus are probably larger, but this is what we can find using the existing subject headings.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Classification\n",
    "\n",
    "Now that we have target labels, we can use them to train a *supervised classifier* to determine genre categories. Unlike with K-means clustering, where we simply create a model and plug in the entire dataset, we need to split our data into a *training set*, which we use to help our model learn, and a *test set*, which we use to see how the model did. In our case, we'll split our data approximately in half, using just over half of the plays for training and reserving the rest for testing. \n",
    "\n",
    "We need to split both the feature set (denoted by a capital X) and the target labels (denoted by a lowercase y). Luckily, scikit-learn does all of this for us with its `train_test_split()` function.\n",
    "\n",
    "Once the data is split, we can choose a model to train. In this case, the method I've chosen is logistic regression (logit). As I mentioned before, logit is quite an old method for classification, and it is useful in part because it is easy to explain and provides results that (as we shall see) are easier to interpret than newer methods like neural networks. Logistic regression uses a logistic function to draw an s-shaped [sigmoid curve](https://en.wikipedia.org/wiki/Sigmoid_function) that takes any value and converts it to a value between 0 and 1. The closer the value is to 0 or 1, the more closely it belongs in one category or another. Because of this 0-or-1, one-or-the-other feature, logit was originally only a *binary* classifier: it could only tell if something was in one of just two categories. However, we can use multiclass or multinomial logistic regression, and the model will predict all three of our classes at once.\n",
    "\n",
    "In the code below, we'll split the data automatically, create a logistic regression model, and \"fit\" that model using the training data. Then, we'll run the model to *predict* categories for the texts in the test set. In the end, we can get accuracy scores, as well as a list of the texts in the test set with their real and predicted genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.7540983606557377\n",
      "\n",
      "Results of this run:\n",
      "\n",
      "Play Title | Actual Genre | Predicted Genre\n",
      "A41058 | neither | neither\n",
      "A23770 | neither | neither\n",
      "A71231 | neither | neither\n",
      "B05875 | poetry | poetry\n",
      "A37237 | history | neither\n",
      "A47095 | neither | neither\n",
      "A50075 | history | neither\n",
      "B03489 | poetry | poetry\n",
      "B04154 | poetry | poetry\n",
      "A23912 | neither | neither\n",
      "B06375 | poetry | poetry\n",
      "B03763 | neither | neither\n",
      "A67335 | poetry | poetry\n",
      "A27397 | neither | neither\n",
      "A41072 | neither | neither\n",
      "A47379 | neither | neither\n",
      "A41527 | neither | neither\n",
      "A57634 | neither | neither\n",
      "A48909 | neither | neither\n",
      "A64918 | history | history\n",
      "A60948 | neither | neither\n",
      "A44478 | neither | poetry\n",
      "A32751 | history | neither\n",
      "A62436 | neither | neither\n",
      "A67572 | neither | neither\n",
      "A61956 | neither | neither\n",
      "A86466 | neither | history\n",
      "B04338 | poetry | neither\n",
      "A32233 | neither | history\n",
      "B03114 | poetry | poetry\n",
      "A48218 | neither | neither\n",
      "A55410 | neither | neither\n",
      "A54418 | neither | neither\n",
      "A39938 | neither | history\n",
      "A46137 | history | history\n",
      "A46196 | history | history\n",
      "A64258 | neither | neither\n",
      "A32503 | neither | history\n",
      "A44938 | neither | neither\n",
      "A52519 | poetry | poetry\n",
      "B05479 | neither | history\n",
      "A59229 | neither | neither\n",
      "A43741 | neither | neither\n",
      "A39345 | poetry | poetry\n",
      "A95297 | history | neither\n",
      "A38741 | history | history\n",
      "A59325 | neither | poetry\n",
      "A87330 | history | neither\n",
      "A53911 | history | poetry\n",
      "A63767 | neither | neither\n",
      "A35114 | neither | neither\n",
      "A37291 | neither | neither\n",
      "A40151 | neither | neither\n",
      "B04364 | poetry | poetry\n",
      "A60606 | poetry | poetry\n",
      "A35608 | history | neither\n",
      "A47545 | history | history\n",
      "B05318 | neither | neither\n",
      "A59614 | neither | neither\n",
      "A36272 | neither | neither\n",
      "A32314 | history | history\n",
      "A61867 | neither | neither\n",
      "A29110 | neither | neither\n",
      "A46152 | history | history\n",
      "A44594 | neither | neither\n",
      "A56039 | neither | neither\n",
      "A81069 | history | poetry\n",
      "B02123 | neither | history\n",
      "A38792 | neither | neither\n",
      "A37096 | history | neither\n",
      "A29439 | neither | history\n",
      "A66777 | poetry | neither\n",
      "A39974 | neither | neither\n",
      "A45206 | neither | neither\n",
      "A47547 | neither | history\n",
      "A65702 | neither | neither\n",
      "A57421 | neither | neither\n",
      "B04360 | poetry | poetry\n",
      "A48797 | neither | neither\n",
      "A66579 | neither | neither\n",
      "B02089 | neither | history\n",
      "A63952 | neither | history\n",
      "A61207 | neither | neither\n",
      "B06022 | poetry | poetry\n",
      "B03631 | history | neither\n",
      "A46193 | history | history\n",
      "A91186 | history | neither\n",
      "A50777 | neither | neither\n",
      "A79623 | neither | neither\n",
      "A51346 | poetry | poetry\n",
      "B04701 | poetry | poetry\n",
      "A50520 | neither | neither\n",
      "A32559 | neither | history\n",
      "A42533 | poetry | poetry\n",
      "A32967 | neither | neither\n",
      "A93280 | neither | neither\n",
      "A46087 | history | history\n",
      "A51130 | poetry | poetry\n",
      "A61891 | neither | neither\n",
      "A75960 | neither | neither\n",
      "B03106 | poetry | poetry\n",
      "A54070 | neither | neither\n",
      "A93278 | poetry | poetry\n",
      "A63951 | neither | history\n",
      "A44879 | poetry | poetry\n",
      "A32555 | history | history\n",
      "A61206 | neither | neither\n",
      "A31237 | history | neither\n",
      "A41053 | neither | neither\n",
      "A39482 | neither | neither\n",
      "A77803 | neither | neither\n",
      "A39442 | neither | neither\n",
      "A80816 | neither | neither\n",
      "A59168 | neither | neither\n",
      "A53818 | neither | neither\n",
      "A51877 | neither | history\n",
      "A39246 | poetry | poetry\n",
      "B05057 | poetry | neither\n",
      "A41958 | neither | neither\n",
      "A31229 | history | history\n",
      "B03672 | poetry | poetry\n",
      "A29017 | neither | neither\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(readable_results, targets, test_size=0.45, random_state=42)\n",
    "lr = LogisticRegression(random_state=0, solver='lbfgs', penalty='none')\n",
    "clf = lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "# evaluate accuracy\n",
    "print(\"Accuracy score:\", accuracy_score(y_test, y_pred, normalize=True, sample_weight=None))\n",
    "print()\n",
    "print(\"Results of this run:\\n\")\n",
    "print(\"Play Title | Actual Genre | Predicted Genre\")\n",
    "for title, real, predicted in zip(X_test.index, y_test, y_pred):\n",
    "    print(f\"{title} | {real} | {predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show us a few things about how the logistic regression model did. First, the *accuracy score* shows the percentage of texts in the test set that were labeled correctly in this run of the model. 75% accuracy is pretty good for a first attempt!\n",
    "\n",
    "And the results themselves show that the model got things mostly right. By browsing the list above, we can see that most of the expected and predicted values match.\n",
    "\n",
    "But there's a catch: the majority of our texts are in the \"neither\" category, i.e. they're neither poetry nor history. If the model guesses \"neither\" most of the time, it's likely to be right most of the time. The question, then, is: how well did the model *actually* do at identifying the texts in these very different categories? There are a few ways of finding out.\n",
    "\n",
    "## Assessing a Classifier\n",
    "\n",
    "After training a classifier, we naturally want to know if it worked. One way we can do this is by scanning a list of results, as above. We can look at the texts that are listed and find out if the mistakes the model made were interesting: perhaps some of the texts are mislabeled and the model is right that they really *are* poetry or history. And it's important to do this kind of close-reading check for any model. A humanities researcher may care as much about what a model gets wrong as what it gets right: the point isn't training a computer to do a task for us, it's what we learn about our corpus by going through the process of training.\n",
    "\n",
    "And to that end, in addition to examining examples, there are a range of mathematical ways to assess a classifier. We already calculated an accuracy score in the code above, which tells us how often the classifier picked the correct category. However, this is only the accuracy result for the data split in just one way and run just once. How does the model do if we split up the data differently?\n",
    "\n",
    "The *cross validation score* answers this question by running the model several times with differently split data. We can use a technique called [*k*-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation), where *k* refers to any possible number. This method splits the data up differently *k* number of times and calculates the average accuracy across the differently-split runs of the classifier. Maybe in one random run, the data winds up split in such a way that the model is much more accurate than it is every other time it's run. Cross-validation accounts for this and gives us a sense of how well the model does no matter how the data is split. We can see that the cross validation shows that this particular run of the model is fairly close to the expected result.\n",
    "\n",
    "With `sklearn`, running *k*-fold cross-validation is as simple as one line of code. We'll do 5-fold cross-validation by setting `cv=5`. This means that `sklearn` will run the model 5 different ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score: 0.7880503144654087\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross validation score:\", np.mean(cross_val_score(lr, readable_results, targets, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results may put our minds somewhat at ease. The classifier actually performs slightly better in cross-validation than it did in our test run: about 79%. But we still don't know how the model performed on our three different categories.\n",
    "\n",
    "In addition to cross-validation, we can also assess a model's accuracy using a *confusion matrix*, a chart that shows how often predicted values matched expected values. In the code below we'll generate the confusion matrix for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe93b380970>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm_df = pd.DataFrame(cm, columns=clf.classes_, index=clf.classes_)\n",
    "f, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.heatmap(cm_df,annot=True,cmap='Greens',linewidths=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this confusion matrix, predicted values (the output of the model) are shown horizontally and expected values (what was in the original data) are shown vertically. When the two match, we can see the number of *true positives*, places where the predicted value matched the expected value. Those are the darker squares along the diagonal from top left to bottom right. Our model got history right 11 times, poetry right 21 times, and correctly predicted when a text was neither 60 times. \n",
    "\n",
    "The confusion matrix also shows false results---places where the predicted value did not match the expected value. Like we saw in the list above, the model miscategorized 3 texts originally labeled \"neither\" as poetry, for example. This could be a blind spot in our model, or it could be that we have more poetry texts in our data than we originally thought! We would have to examine the texts to find out.\n",
    "\n",
    "What we can see from this matrix is that the model struggled the most with the history category. Look at the top row and the left-most column, where the history texts are categorized. Things are less neatly sorted into the top-left square of true positives as we'd like to see. Instead, history texts are sometimes misclassified as \"neither,\" and \"neither\" texts are sometimes misclassified as history.\n",
    "\n",
    "We can quantify what we're seeing in the confusion matrix with the final way of assessing our classifier: `sklearn`'s classification report. Here's the classification report that goes with the above matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     history       0.46      0.48      0.47        23\n",
      "     neither       0.82      0.80      0.81        75\n",
      "      poetry       0.84      0.88      0.86        24\n",
      "\n",
      "    accuracy                           0.75       122\n",
      "   macro avg       0.71      0.72      0.71       122\n",
      "weighted avg       0.76      0.75      0.76       122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=clf.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report gives us scores for the [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) of the classifier across each category. Precision is the ratio of true positives (correct guesses of a text's category) to the number of total times it guessed that category. Recall is the ratio of true positives to the total number of texts in that category that existed in the dataset. So precision tells us how often the model guessed correctly while recall tells us how often the model found the right thingâ€”two different views of a similar task. (If this is still confusing to you, I highly recommend the examples and visualizations in the [Wikipedia article on this topic](https://en.wikipedia.org/wiki/Precision_and_recall).)\n",
    "\n",
    "The [F1-score](https://en.wikipedia.org/wiki/Precision_and_recall#F-measure) combines these two measures, and \"support\" just shows the number of times that category appeared in our dataset (i.e. the total number of texts with that label).\n",
    "\n",
    "How did our classifier do, according to this report? It did a decent job with the \"neither\" category, which makes sense since \"neither\" was the largest category. And it did really well with poetry! Especially with recall: if there was poetry in a text, our classifier could find it. Given that verse is distinctive compared to a nebulous genre like \"history,\" it makes sense that this was the classifier's strength.\n",
    "\n",
    "And we can can see that the history category is where the classifier really struggled. In fact, it's what's dragging down our overall accuracy score.  Both precision and recall are less than 50%: the classifier can neither find historical texts nor guess their label correctly. It's worse than a coin flip!\n",
    "\n",
    "The classification report clarifies trends that were harder to see in our other methods of model assessment. Namely, one category (history) is badly suited to this method. Perhaps we could create a classifier that's quite good at identifying poetry texts, but we need to go back to the drawing board on history. This makes sense since, as I mentioned previously, \"history\" isn't a very well-defined genre in this corpus. Instead it's a subject label applied to lots of different kinds of texts. If we're really interested in correctly classifying historical texts, we may want to hand label a sample and start over with our model.\n",
    "\n",
    "Just as we discovered with K-means clustering, it's easy to run these models but harder to assess and interpret their results. It requires a knowledge of the task at hand, a general idea of what's in the corpus, and a willingness to drill down to specific examples. The example we worked through above didn't work perfectly, and that makes it a good object lesson in supervised classification. You won't always find the right model or the right categories on the first try, and that's okay. By applying your knowledge of the corpus and some general principles of how classifiers work, you can still learn something from imperfect classifiers and work toward a model that does the job."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
