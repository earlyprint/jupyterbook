
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>5. Word Embeddings &#8212; EarlyPrint + Python</title>
    
  <link rel="stylesheet" href="_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="6. Statistical Modeling" href="modeling.html" />
    <link rel="prev" title="4. Text Similarity" href="similarity.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/eplogo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">EarlyPrint + Python</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   EarlyPrint + Python
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="ep_xml.html">
   1. Parsing
   <em>
    EarlyPrint
   </em>
   XML Texts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tf_idf.html">
   2. Exploring Vocabulary Using Tf-Idf
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="metadata.html">
   3. Working with Metadata, Creating Text Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="similarity.html">
   4. Text Similarity
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   5. Word Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="modeling.html">
   6. Statistical Modeling
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/word2vec.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/earlyprint/jupyterbook"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/earlyprint/jupyterbook/issues/new?title=Issue%20on%20page%20%2Fword2vec.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/earlyprint/jupyterbook/master?urlpath=tree/word2vec.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#select-a-corpus-and-tokenize-sentences">
   5.1. Select a Corpus and Tokenize Sentences
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#train-a-model-and-find-similar-words">
   5.2. Train a Model and Find Similar Words
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#accessing-and-visualizing-vectors">
   5.3. Accessing and Visualizing Vectors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#visualize-word-similarity">
   5.4. Visualize Word Similarity
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="word-embeddings">
<h1><span class="section-number">5. </span>Word Embeddings<a class="headerlink" href="#word-embeddings" title="Permalink to this headline">¶</a></h1>
<p>Many of the improvements and extensions we have in mind for the next stage of <em>EarlyPrint</em> involve word embeddings. Using popular algorithms like Word2Vec, which we’ll use in this tutorial, word embeddings create numeric representations for each word in a text corpus based on how frequently that word co-occurs alongside other words. Using word embeddings, it’s possible to compute the semantic relationship among words in a corpus, to find <strong>how words are related</strong>.</p>
<p>Word embeddings have lots of different downstream uses for machine learning and text analysis tasks; for instance, they’re an important building block for state-of-the-art natural language processing models. In this tutorial we’ll focus on the basics: training a Word2Vec model and using the model to identify and visualize similar words.</p>
<p>You can decide how deep you’d like to go into the weeds of the ways Word2Vec actually produces its similarities. For a first introduction, I recommend <a class="reference external" href="https://jalammar.github.io/illustrated-word2vec/">Jay Alammar’s <em>The Illustrated Word2Vec</em></a>, which includes lots of helpful visualizations as it explains the basics. In this tutorial, we’ll focus on training Word2Vec in Python and interpreting the results, rather than reviewing the underlying concepts.</p>
<p>Let’s begin by importing the necessarily libraries:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lxml</span> <span class="kn">import</span> <span class="n">etree</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>
<span class="kn">import</span> <span class="nn">glob</span><span class="o">,</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="select-a-corpus-and-tokenize-sentences">
<h2><span class="section-number">5.1. </span>Select a Corpus and Tokenize Sentences<a class="headerlink" href="#select-a-corpus-and-tokenize-sentences" title="Permalink to this headline">¶</a></h2>
<p>Word2Vec creates vectors based on which words are near one another. We’ll train our model on lists of words from a selected corpus, and we’ll use the same corpus we’ve been using all along: the <em>EarlyPrint</em> texts published in 1666.</p>
<p>We’re using the <a class="reference external" href="https://radimrehurek.com/gensim/models/word2vec.html">Gensim</a> library to create our Word2Vec model. Gensim accepts several different types of input, but we’ll focus on giving it lists of word tokens. You could create a list of all the tokens in a text and pass them directly to Gensim, but Gensim prefers that you give it each sentence separately. This is because sentence boundaries often contain information about word relationships: the last word of a given sentence and the first word of the next one don’t have the same relationship as two words in the same sentence.</p>
<p>Below we create a function for finding the lemmas in every sentence of our texts, which then collects those sentences as individual lists. For more detail on how this code works, refer to <a class="reference external" href="https://earlyprint.org/jupyterbook/ep_xml.html#step-4-lines-stanzas-and-sentences">our XML tutorial</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_sentences</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">etree</span><span class="o">.</span><span class="n">XMLParser</span><span class="p">(</span><span class="n">collect_ids</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">tree</span> <span class="o">=</span> <span class="n">etree</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">parser</span><span class="p">)</span>
    <span class="n">xml</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">getroot</span><span class="p">()</span>
    <span class="n">new_sentence</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># An empty list for the first sentence</span>
    <span class="c1"># Loop through every tag</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">xml</span><span class="o">.</span><span class="n">iter</span><span class="p">(</span><span class="s1">&#39;{*}w&#39;</span><span class="p">):</span>
        <span class="n">previous</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">getprevious</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">previous</span> <span class="o">!=</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">previous</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;unit&#39;</span><span class="p">,</span> <span class="n">previous</span><span class="p">)</span> <span class="o">==</span> <span class="s1">&#39;sentence&#39;</span><span class="p">:</span>
            <span class="n">all_sentences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_sentence</span><span class="p">)</span>
            <span class="n">new_sentence</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">new_sentence</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;lemma&#39;</span><span class="p">,</span> <span class="n">word</span><span class="o">.</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nsmap</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;tei&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.tei-c.org/ns/1.0&#39;</span><span class="p">}</span>
<span class="n">files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;1666_texts/*.xml&#39;</span><span class="p">)</span>
<span class="n">all_sentences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">files</span><span class="p">:</span>
    <span class="n">get_sentences</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="train-a-model-and-find-similar-words">
<h2><span class="section-number">5.2. </span>Train a Model and Find Similar Words<a class="headerlink" href="#train-a-model-and-find-similar-words" title="Permalink to this headline">¶</a></h2>
<p>Now that we have a list of all the lemmas in all the sentences in our texts, we’re ready to train our Word2Vec model. This can be done with the simple one-line command below.</p>
<p>The parameter <code class="docutils literal notranslate"><span class="pre">min_count</span></code> refers to the minimum number of times a word must appear in the corpus in order to be part of the model. For the sake of speed, I’m eliminating all words that appear less than 2 times. (And since the eliminated words appear only once, the resulting vectors wouldn’t be very reliable anyway: not enough examples of adjacent words.)</p>
<p>The parameter <code class="docutils literal notranslate"><span class="pre">window</span></code> refers to the “sliding window” that Word2Vec pulls across a sentence to determine if words are near each other. The default window is 5 words. In general, Word2Vec gives better results in a very large corpus, when there are lots of instances of every word. For our <em>EarlyPrint</em> applications of Word2Vec, we’ll train the model on the full corpus. But for this sample one-year corpus, let’s shrink the window to just 4 words, which will generate more windows across our smaller corpus.</p>
<p>Once we’ve selected parameters we can train our Word2Vec model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word2vec</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">all_sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Word2Vec outputs a model with a <code class="docutils literal notranslate"><span class="pre">wv</span></code> object that contains lots of information about the word embeddings. For example, every single word in the corpus is stored in the <code class="docutils literal notranslate"><span class="pre">vocab</span></code> attribute:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">word2vec</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">)[:</span><span class="mi">50</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;the&#39;, &#39;dutch&#39;, &#39;gazette&#39;, &#39;or&#39;, &#39;sheet&#39;, &#39;of&#39;, &#39;wildfire&#39;, &#39;that&#39;, &#39;fire&#39;, &#39;fleet&#39;, &#39;i&#39;, &#39;will&#39;, &#39;tell&#39;, &#39;you&#39;, &#39;not&#39;, &#39;aetna&#39;, &#39;flame&#39;, &#39;nor&#39;, &#39;troy&#39;, &#39;long&#39;, &#39;ago&#39;, &#39;have&#39;, &#39;fill&#39;, &#39;world&#39;, &#39;with&#39;, &#39;noise&#39;, &#39;romance&#39;, &#39;history&#39;, &#39;do&#39;, &#39;age&#39;, &#39;before&#39;, &#39;who&#39;, &#39;obsequy&#39;, &#39;be&#39;, &#39;sing&#39;, &#39;by&#39;, &#39;laureate&#39;, &#39;pen&#39;, &#39;which&#39;, &#39;story&#39;, &#39;can&#39;, &#39;parallel&#39;, &#39;rupert&#39;, &#39;duke&#39;, &#39;albemarle&#39;, &#39;and&#39;, &#39;homes&#39;, &#39;rest&#39;, &#39;send&#39;, &#39;those&#39;]
</pre></div>
</div>
</div>
</div>
<p>Using the <code class="docutils literal notranslate"><span class="pre">wv</span></code> object, it’s simple to retrieve the words most similar to a particular word of your choice. Looking at the first 50 words in the corpus above, we’ll choose a word with special resonance for 1666, the year of the Great Fire of London: “flame.” Let’s find the words most similar to the word “flame,” where similarity refers to the likelihood that the word would appear in contexts measurably like the ones in which “flame” appears.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">word2vec</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;flame&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;cloud&#39;, 0.9317228198051453), (&#39;fire&#39;, 0.9068721532821655), (&#39;smoke&#39;, 0.8920124769210815), (&#39;rock&#39;, 0.8740819692611694), (&#39;burn&#39;, 0.8726823329925537), (&#39;flood&#39;, 0.8679195046424866), (&#39;chain&#39;, 0.8679168224334717), (&#39;dust&#39;, 0.8658936023712158), (&#39;ash&#39;, 0.8653439879417419), (&#39;wind&#39;, 0.8642510175704956)]
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">most_similar()</span></code> function gives the top ten most similar words to the word you selected. The similar words to “flame”—“fire,” “smoke”, “ash”—make a lot of sense.</p>
<p>The values given with each word are its cosine similarity to the source word. (See our <a class="reference external" href="https://earlyprint.org/jupyterbook/similarity.html">Similarity tutorial</a> for more about this.)</p>
<p>If you had a list of words you were particularly interested in, perhaps organized around a theme, you could easily look at the most similar words to each one and begin to populate the semantic field of the topic that interests you.</p>
</div>
<div class="section" id="accessing-and-visualizing-vectors">
<h2><span class="section-number">5.3. </span>Accessing and Visualizing Vectors<a class="headerlink" href="#accessing-and-visualizing-vectors" title="Permalink to this headline">¶</a></h2>
<p>But we can do more with word embeddings than simply find similar words. Word2Vec creates a vector, a string of numerical values for each word, that we can access with the <code class="docutils literal notranslate"><span class="pre">wv</span></code> object.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">word2vec</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;flame&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 0.7145108  -0.12516643 -0.29721773  0.18529506 -0.1267187   0.81016743
 -0.2698178  -0.58050805 -1.3886431  -0.26384622 -1.0610106  -0.8030535
 -0.09151758 -0.5355215  -0.64998287 -0.28354886  0.9338349  -0.01685463
  0.58463025 -0.46613577 -0.44858545 -0.47312522  0.6826598  -0.35454983
  0.78752154 -0.37928095  0.6372427   0.87677026  0.05423938  0.47669122
 -0.9627098  -0.7284044   0.5599645  -0.42624342  0.37250212  0.1257531
  0.07453073  0.5000426   1.0599877   0.04867575  0.32578316 -1.105912
  0.6712924  -0.48948392 -0.86583376 -0.04262859 -0.5072744   0.95682144
 -0.459579    0.33388668  0.87895316  0.8516653   0.56441784  0.5792019
  0.0896054   0.915266    0.5191485  -0.6310904  -0.24730214 -0.4670559
 -1.2663445  -0.95956767 -0.7953577  -0.05179178 -0.54349494 -0.12064929
 -0.9203471  -0.63457096 -0.43178302  0.49809232 -0.420157    0.01884362
  0.6277516  -0.72252    -0.23130576  0.6371168   0.5209998   0.37248883
 -0.5978805   0.32759762 -0.00362215  0.4915843   0.30044505  0.04706769
 -0.17561153 -0.765087    1.1106958  -0.668933    0.35436007  0.09042933
  0.63939345 -0.01185623  0.19130613  0.72370106 -0.3960629   0.6229613
 -0.9803799  -0.14198333  0.08416303  0.07705496]
</pre></div>
</div>
</div>
</div>
<p>Each word in the text has a vector of the same length of 100 features, or dimensions, though the value of each feature will be different for every word. Using these 100-dimension vectors, we can recreate some of the illustrations from Alammar’s <a class="reference external" href="https://jalammar.github.io/illustrated-word2vec/">The Illustrated Word2Vec</a>.</p>
<p>Let’s start with “flame” and three similar words: “cloud,” “fire,” and “smoke.” We can put the vectors of each of these words into a <code class="docutils literal notranslate"><span class="pre">pandas</span></code> DataFrame and then visualize them as a heatmap.</p>
<p><em>n.b. For this step and the next one, we are using <a class="reference external" href="https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm">L2-normalized</a> vectors accessed through the <code class="docutils literal notranslate"><span class="pre">vectors_norm</span></code> attribute. This simply gives us comparable numbers regardless of magnitude, or how frequently a single word appears.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wordlist</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;flame&quot;</span><span class="p">,</span> <span class="s2">&quot;cloud&quot;</span><span class="p">,</span> <span class="s2">&quot;fire&quot;</span><span class="p">,</span> <span class="s2">&quot;smoke&quot;</span><span class="p">]</span> <span class="c1"># The words we&#39;ve selected</span>
<span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word2vec</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">index2entity</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">wordlist</span><span class="p">]</span> <span class="c1"># The numerical indices of those words</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">word2vec</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vectors_norm</span><span class="p">[</span><span class="n">indices</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">wordlist</span><span class="p">)</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f18cbf5a8b0&gt;
</pre></div>
</div>
<img alt="_images/word2vec_14_1.png" src="_images/word2vec_14_1.png" />
</div>
</div>
<p>Like in Jalammar’s illustrations for “king,” “man,” and “woman,” it’s easy to see why these words have high similarity scores. Parts of the vectors for each word have high (red) values and low (blue) values, and these roughly matchup from word to word.</p>
<p>We could visualize every single word this way, but the resulting chart would be very hard to read. Instead, let’s try other visualization methods.</p>
</div>
<div class="section" id="visualize-word-similarity">
<h2><span class="section-number">5.4. </span>Visualize Word Similarity<a class="headerlink" href="#visualize-word-similarity" title="Permalink to this headline">¶</a></h2>
<p>By default Word2Vec reduces a word to a vector of 100 features, or 100 dimensions. We can’t visualize a 100-dimensional space: the best we can do is two or three. To go from 100 dimensions to just 2, we need a method for feature reduction.</p>
<p>We’ll use one of the most common feature reduction methods: Principal Component Analysis. PCA reduces a high-dimensional space into just a few “principal components” that attempt to account for most of the variance in the data. In many applications, such as when <a class="reference external" href="https://earlyprint.org/jupyterbook/similarity.html">calculating similarity</a>, it’s best to work in the higher dimensional space directly. But PCA is a very useful tool for visualization: just remember that you’re looking at a reduced representation of the data rather than the  original data.</p>
<p>Let’s import PCA from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll initialize PCA with just 2 principal components, since we want to graph in 2 dimensions. We want to give PCA the vectors for every word in our corpus: remember that we can access a complete list of words using <code class="docutils literal notranslate"><span class="pre">wv.vocab</span></code>.</p>
<p>Below we’ll run PCA and put the results in a DataFrame:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca_results</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">word2vec</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vectors_norm</span><span class="p">)</span>
<span class="n">pca_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pca_results</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">word2vec</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;pc1&quot;</span><span class="p">,</span><span class="s2">&quot;pc2&quot;</span><span class="p">])</span>
<span class="n">pca_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pc1</th>
      <th>pc2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>the</th>
      <td>1.088081</td>
      <td>0.064894</td>
    </tr>
    <tr>
      <th>dutch</th>
      <td>1.065645</td>
      <td>0.039468</td>
    </tr>
    <tr>
      <th>gazette</th>
      <td>1.097556</td>
      <td>-0.010632</td>
    </tr>
    <tr>
      <th>or</th>
      <td>1.041847</td>
      <td>0.086949</td>
    </tr>
    <tr>
      <th>sheet</th>
      <td>1.056707</td>
      <td>0.050866</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>whittall</th>
      <td>0.129968</td>
      <td>-0.250855</td>
    </tr>
    <tr>
      <th>noyes</th>
      <td>0.106204</td>
      <td>-0.265949</td>
    </tr>
    <tr>
      <th>grone</th>
      <td>-0.034138</td>
      <td>-0.053683</td>
    </tr>
    <tr>
      <th>surman</th>
      <td>-0.123803</td>
      <td>-0.199232</td>
    </tr>
    <tr>
      <th>brawn</th>
      <td>0.047043</td>
      <td>0.225559</td>
    </tr>
  </tbody>
</table>
<p>70551 rows × 2 columns</p>
</div></div></div>
</div>
<p>Now that we have a DataFrame with just 2 dimensions, “pc1” and “pc2,” we can create a scatterplot of every word, where our principal components are the x and y axes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pca_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;pc1&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;pc2&#39;</span><span class="p">,</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;scatter&quot;</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f18c8043b80&gt;
</pre></div>
</div>
<img alt="_images/word2vec_20_1.png" src="_images/word2vec_20_1.png" />
</div>
</div>
<p>The graph above gives us a general sense of where each word sits in relation to all the others, but it’s not very informative as a mass of blue dots.</p>
<p>Let’s use this graph as a base, but get rid of all the dots and just show the labels for the four words we care about.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">pca_df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;pc1&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;pc2&#39;</span><span class="p">,</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;scatter&quot;</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span><span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">txt</span> <span class="ow">in</span> <span class="n">pca_df</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">txt</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;flame&quot;</span><span class="p">,</span> <span class="s2">&quot;cloud&quot;</span><span class="p">,</span> <span class="s2">&quot;fire&quot;</span><span class="p">,</span> <span class="s2">&quot;smoke&quot;</span><span class="p">]:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">pca_df</span><span class="o">.</span><span class="n">pc1</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">txt</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">pca_df</span><span class="o">.</span><span class="n">pc2</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">txt</span><span class="p">]</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/word2vec_22_0.png" src="_images/word2vec_22_0.png" />
</div>
</div>
<p>This is much better. We can see how the words are positioned relative to one another. Though we might have expected to seem them next to each other, remember that PCA is only showing us a reduced representation of our high-dimensional data. Still, we can see that “smoke” and “flame” are closer to each other than “cloud” and “fire.” And all four words are almost in a horizontal line across the graph, suggesting they’re quite similar according to principal component 2 (the y-axis), but perhaps less similar according to PC1 (the x-axis). Finally, by comparing the last graph to this one, we can also see where these words fit in areas of semantic density (lots of similar words) versus sparseness (words with more distinct meanings).</p>
<p>As text similarity increased our sense of text-level relationships in the Similarity tutorial, Word2Vec gives us a clearer (but not complete) sense of word-level relationships. Stacked with other methods, Word2Vec can be used to explore themes and subjects, to help computers and human readers make sense of semantic distinctions, and to drive complex language-based machine learning algorithms.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="similarity.html" title="previous page"><span class="section-number">4. </span>Text Similarity</a>
    <a class='right-next' id="next-link" href="modeling.html" title="next page"><span class="section-number">6. </span>Statistical Modeling</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By John R. Ladd<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>